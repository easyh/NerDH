{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Jupiter Notebook zum NerDH Tutorial\n",
    "\n",
    "\n",
    "[Zurück zum Web-Tutorial](https://easyh.github.io/NerDH/tut/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Trainings-, Validierungs- und Testdaten\n",
    "\n",
    "Beim maschinellen Lernen benötigen wir einen Trainingsdatensatz, um ein Modell richtig zu trainieren und einen Testdatensatz, um das Modell zu bewerten.  Der Datensatz [`taggedData.json`](https://github.com/easyh/NerDH/blob/main/data/datensets/taggedData.json) umfasst den komplett annotierten Text [**München 1611**](https://hainhofer.hab.de/reiseberichte/muenchen1611?v={%22view%22:%22info%22}). Bei **unüberwachten Lernmethoden** wird dieser Datensatz in der Regel in mindestens drei verschiedene Datensätze unterteilt: **Training-, Validierung- und Testdaten**. In unserem Fall übernehmen die Trainingsdaten ([`testData.json`](https://github.com/easyh/NerDH/blob/main/data/datensets/taggedData.json)) bei uns die annotierten Daten aus dem Text [**München 1603**](https://hainhofer.hab.de/reiseberichte/muenchen1603). Die Aufteilung unserer Daten sieht wie folgt aus:\n",
    "<div>\n",
    "<img src=\"../nerdh_tutorial/docs/img/datenset_lg.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um jetzt unseren großen Datensatz [`taggedData.json`](https://github.com/easyh/NerDH/blob/main/data/datensets/taggedData.json) in zwei Datensets aufzuteilen, lesen wir diesen zunächst ein und speichern nur die Einträge von  `annotations` in der Variablen `TAGGED_DATA`, damit wir die Einträge zählen können. Danach ermitteln wir die Grenze (80:20), damit wir den ursprünglichen Datensatz in kleinere Datensätze zu je 80% und 20% aufteilen können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "f = open('../data/datensets/taggedData.json')\n",
    "data = json.load(f)\n",
    "TAGGED_DATA = data['annotations']\n",
    "\n",
    "print(len(TAGGED_DATA)*0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bevor wir den Datensatz an den ermittelten Grenzen in die drei Datensets aufteilen, mischen wir die Einträge noch einmal durch, damit die Verteilung zufällig ist. Hier lassen wir uns dann jeweils die Länge ausgeben, um das Ergebnis zu überprüfen. Zusätzlich lassen wir uns auch noch die Größe des Testdatensatz ausgeben, um zu überprüfen, ob die 70:20:10 Verteilung gewährleistet ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "random.shuffle(TAGGED_DATA)\n",
    "train_data = TAGGED_DATA[:697]\n",
    "val_data = TAGGED_DATA[697:]\n",
    "\n",
    "print(\"Trainingsdaten: \" + str(len(train_data)))\n",
    "print(\"Validierungsdaten: \" + str(len(val_data)))\n",
    "\n",
    "#Zum vergleich, lassen wir uns auch die Göße von unseren Testdaten ausgeben\n",
    "f = open('../data/datensets/testData.json')\n",
    "data = json.load(f)\n",
    "test_data = data['annotations']\n",
    "print(\"Testdaten: \" + str(len(test_data)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anschließend speichern wir die Datensets im JSON-Format ab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('../data/datensets/trainData.json', 'w', encoding='utf-8') as train: \n",
    "    json.dump(train_data, train, ensure_ascii=False, indent=4)\n",
    "with open ('../data/datensets/valuationData.json', 'w', encoding='utf-8') as val: \n",
    "    json.dump(val_data, val, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da wir weiter oben nur die Einträge aus der Klasse  `annotations` der JSON-Datei übernommen haben, müssen wir jetzt noch einmal manuell bei `trainData.json` sowie `validationData.json` die `classes` hinzufügen, damit unsere Kategorien für die Entitäten nicht verloren gehen. Dazu setzen wir an den Anfang des Dokuments folgendes und ans Ende eine `}` um das Dokument zu schließen.\n",
    "\n",
    "```json\n",
    "{\"classes\": [\"PERSON\",\"ORT\", \"ORGANISATION\",\"OBJEKT\",\"ZEIT\"],\n",
    "    \"annotations\": \n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt müssen die Datensets im `JSON`-Format nur noch ins `spaCy`-Format konvertiert werden. Dafür importieren wir zunächst die entsprechenden Bibliotheken und das mittlere Sprachmodell von `spaCy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "#neues spacy Model laden\n",
    "nlp = spacy.load(\"de_core_news_md\") \n",
    "\n",
    "#DocBin Objekt erstellen\n",
    "db = DocBin() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt müssen wir für jedes Datenset nur noch folgenden Code ausführen, damit die Datensets im `spaCy`-Datenformat sind. \n",
    "Der Code ist von [hier](https://towardsdatascience.com/using-spacy-3-0-to-build-a-custom-ner-model-c9256bea098)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trainingsdaten\n",
    "f = open('../data/datensets/trainData.json')\n",
    "TRAIN_DATA = json.load(f)\n",
    "\n",
    "for text, annot in tqdm(TRAIN_DATA['annotations']): \n",
    "    doc = nlp.make_doc(text) \n",
    "    ents = []\n",
    "    for start, end, label in annot[\"entities\"]:\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "        if span is None:\n",
    "            print(\"Skipping entity\")\n",
    "        else:\n",
    "            ents.append(span)\n",
    "    doc.ents = ents \n",
    "    db.add(doc)\n",
    "\n",
    "#Docbin Objekt speichern\n",
    "db.to_disk(\"../data/datensets/trainData.spacy\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Valuiierungsdaten\n",
    "f = open('../data/datensets/valuationData.json')\n",
    "VAL_DATA = json.load(f)\n",
    "\n",
    "for text, annot in tqdm(VAL_DATA['annotations']): \n",
    "    doc = nlp.make_doc(text) \n",
    "    ents = []\n",
    "    for start, end, label in annot[\"entities\"]:\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "        if span is None:\n",
    "            print(\"Skipping entity\")\n",
    "        else:\n",
    "            ents.append(span)\n",
    "    doc.ents = ents \n",
    "    db.add(doc)\n",
    "\n",
    "#Docbin Objekt speichern\n",
    "db.to_disk(\"../data/datensets/valuationData.spacy\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testdata\n",
    "f = open('../data/datensets/testData.json')\n",
    "TEST_DATA = json.load(f)\n",
    "\n",
    "for text, annot in tqdm(TEST_DATA['annotations']): \n",
    "    doc = nlp.make_doc(text) \n",
    "    ents = []\n",
    "    for start, end, label in annot[\"entities\"]:\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "        if span is None:\n",
    "            print(\"Skipping entity\")\n",
    "        else:\n",
    "            ents.append(span)\n",
    "    doc.ents = ents \n",
    "    db.add(doc)\n",
    "\n",
    "#Docbin Objekt speichern\n",
    "db.to_disk(\"../data/datensets/testData.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damit ist das Preprocessing abgeschlossen und wir können mit dem Training des Modells beginnen.\n",
    "\n",
    "Im Github Repository befinden sich die Datensets im `JSON`- & `spaCy`-Format [**hier**](https://github.com/easyh/NerDH/blob/main/data/datensets) (`data/datensets`)."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
