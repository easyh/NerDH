{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Jupiter Notebook zum NerDH Tutorial\n",
    "\n",
    "\n",
    "[Zurück zum Web-Tutorial](https://easyh.github.io/NerDH/tut/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Textpreprocessing\n",
    "\n",
    "Da wir unser eigenes Modell mit `spaCy` erstellen möchten, müssen unsere Daten mit Goldstandard Anspruch ins richtige Format gebracht werden. Folgendes Beispiel zeigt, wie unsere Daten aussehen müssen. \n",
    "\n",
    "```\n",
    "TRAIN_DATA = [\"TEXT AS A STRING\",{\"entities:\"[(START,END,LABEL)]}]\n",
    "```\n",
    "\n",
    "Es gibt zahlreiche Auszeichnungstools, die für Anwendungen des maschinellen Lernens entwickelt wurden und bei der Annotation von Texten helfen. Wir erstellen unseren Goldstandard mit dem [NER-Annotator](https://tecoholic.github.io/ner-annotator/). Hier kann ein Text im `TXT`-Format importiert, annotiert und dann anschließend im `JSON`-Format exportiert werden. Die `JSON`-Datei enthält dann die annotierten Daten, die in dem für `spaCy` geeigneten Format vorliegen. Die `JSON`-Dateien müssen dann nochmal in `spaCy`-Dateien umgewandelt werden. Das werden wir allerdings erst im nächsten Kapitel machen. \n",
    "\n",
    "**Unser Ziel hier ist es erstmal die Daten zu annotieren, sodass wir eine `JSON`-Datei erhalten.**\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "Bevor wir allerdings mit der Annotation des Goldstandards beginnen wollen wir unseren Text erst noch etwas kennenlernen und vorbereiten. \n",
    "\n",
    "Unsere Texte [**München 1611**](https://hainhofer.hab.de/reiseberichte/muenchen1611?v={%22view%22:%22info%22}) & [**München 1603**](https://hainhofer.hab.de/reiseberichte/muenchen1603) haben wir im `TXT`-Format vorliegen und lesen diese ersteinmal ein und zählen die Zeichen. Wir führen das für jeden Text nacheinander aus. In diesem Notebook wird dies für den Text **München 1611** gemacht.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"../data/texte/Mue1611.txt\",'r', encoding='utf-8') as file: \n",
    "    text = file.read()\n",
    "\n",
    "#da uns um Text viele / aufgefallen sind, entfernen wir diese durch nichts\n",
    "text = text.replace(\"/\", \" \")\n",
    "\n",
    "print(\"Anzahl Zeichen im Text: \" + str(len(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt verwenden wir zwei der `spaCy`-Funktionen, die wir schon kennengelernt haben. Einmal um die Sätze und einmal um die Tokens zu zählen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "doc = nlp(text)\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Anzahl der Tokens ermitteln\n",
    "tokens = []\n",
    "for token in doc: \n",
    "    tokens.append(token)\n",
    "\n",
    "print(\"Anzahl Tokens im Text: \" + str(len(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Anzahl der Sätze ermitteln\n",
    "sentences = []\n",
    "for sent in doc.sents: \n",
    "    sentences.append(sent)\n",
    "\n",
    "print(\"Anzahl Sätze im Text: \" + str(len(sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was wir jetzt noch machen, ist den Text in ein neues Dokument zu speichern. In der ursprünglichen `TXT`-Datei hat unser Text 544 Zeileneinträge, das ist für die Anzahl an hier ermittelten Sätzen ziemlich wenig. Mit Blick auf den nächsten Schritt, in welchem wir das annotierte Datenset in die Datensets anhand der Einträge aufteilen möchten, macht es Sinn, die Anzahl der Einträge zu erhöhen, um eine gleichmäßigere Verteilung zu erhalten. Komplett gleich werden wir hier nicht erreichen, da die Sätze unterschiedliche Längen haben. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/texte/Mue1611_sentences.txt', 'w', encoding='utf-8') as f:\n",
    "    for s in sentences: \n",
    "        if str(s).endswith('.'):\n",
    "            f.write(str(s) + '\\n')\n",
    "        else:\n",
    "             f.write(str(s) + ' ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"../data/texte/Mue1611_sentences.txt\",'r', encoding='utf-8') as file: \n",
    "    text = file.readlines()\n",
    "\n",
    "newtext = []\n",
    "for l in text: \n",
    "    if l.startswith(\" \"):\n",
    "        newl = l.lstrip()\n",
    "        newtext.append(newl)\n",
    "    else: \n",
    "        newtext.append(l)\n",
    "\n",
    "\n",
    "with open('../data/texte/Mue1611_sentences_final.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in newtext:\n",
    "        f.write(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt haben wir in unserem Textdokument 1600 Einträge anstatt 544. 1600 ist auch die Anzahl unserer Sätze. Wirft man einmal einen Blick in die Liste `sentences´ rein, erkennt man schnell, dass viele leere Einträge mit dabei sind. Daher ist unsere Anzahl an Sätzen geringer.\n",
    "\n",
    "Jetzt können wir unsere Datei `Mue1611_sentences_final.txt` bzw. `Mue1603_sentences_final.txt`im [NER-Annotator](https://tecoholic.github.io/ner-annotator/) hochladen und mit der Annotation beginnen."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
